{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "21a08744-c97e-4f2a-a4ca-24b24db7a6ce",
   "metadata": {},
   "source": [
    "# Analyse et Exploration des données S4 : Bases de l’inférence statistique (test d’hypothèse)\n",
    "\n",
    "- Année 2025/2026\n",
    "- Jean Delpech\n",
    "- Classe : B3 IA/Data (Campus Aix-en-Provence)\n",
    "- Dernière mise à jour : janvier 2026"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1918c36-f914-477b-bfeb-5c0c8e6a8b66",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a937c00-e287-4648-8274-987e893da7ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "tips_df = sns.load_dataset(\"tips\")\n",
    "tips_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6180cee0-59f8-4648-88a3-b9f5096b7847",
   "metadata": {},
   "source": [
    "## Derniers rappels sur l’EDA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fc35dce-6bea-4fc9-9cbb-7b6bb49ed599",
   "metadata": {},
   "source": [
    "Le but premier de l’EDA (*Exploratory Data Analysis*) est de se construire une bonne représentation et surtout compréhension des données en relevant des tendances et en détectant les anomalies. C’est la première étape indispensable de la création d’un modèle de *machine learning* ou de l’analyse de données. C’est à partir de là que vous formulerez les hypothèses (sur les relations entre variables) qui guideront votre travail et que vous allez tester.\n",
    "\n",
    "Opérationnellement, il s’agit donc d’une part d’établir des statistiques de bases qui pourront nous aider à décrire les données et les tendances que l’on observe, de générer des *dataviz* pertinentes, et d’autre part de préparer les données pour l’analyse à venir, notamment de les nettoyer (valeurs aberrantes, valeurs manquantes).\n",
    "\n",
    "Une poignée de méthodes sont à connaître absolument car elle nous permettont de réaliser la majeure partie de ce travail à l’aide de la bibliothèque `pandas`, et des bibliothèques de dataviz `seaborn` et `plotly`. Après avoir acquis ainsi une première compréhension des données à l’aide de ces méthodes, vous pourrez achever finement l’EDA en créant des opérations sur mesure pour vos données.\n",
    "\n",
    "* `df.info()` : la première méthode à appeler une fois que vous aurez chargé vos données sous la forme d’un DataFrame. Cette méthode vous permet d’un coup de disposer des informations suivantes :\n",
    "  \n",
    "    * la taille (*shape*) du tableau (lignes / colonnes)\n",
    "    * le nom de chaque colonne (nom de variables *a priori*)\n",
    "    * le nombre de valeurs non-nulles dans chaque colonne\n",
    "    * le datatype des valeurs dans chaque colonne\n",
    "    * la taille en mémoire du DataFrame "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae432458-a505-4b25-bbeb-0098ac15ac0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "tips_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e4e69e1-944b-49de-a0c5-b8d23b909aee",
   "metadata": {},
   "source": [
    "* pour les données numériques/quantitatives : `df.describe()`. Cette méhode va vous fournir les statistiques descriptives de base (nombre, moyenne, écart-type, médiane, min, max, quartiles…) permettant de se représenter la distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3815596c-975e-45c5-94e0-c261acb27197",
   "metadata": {},
   "outputs": [],
   "source": [
    "tips_df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68b84f5a-b9bc-4d9a-b75c-7785b576d17b",
   "metadata": {},
   "source": [
    "* Pour les variables catégorielles/qualitatives :  `df['colonne'].value_counts()`. Cette fonction permet de dénombrer rapidement le nombre d’élément dans chaque catégorie (qui sont alors énumérées par la même occasion). Cela remplace avantageusement le recours aux méthoddes `df.unique()̀` et `df.nunique()̀` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f577e8e-5321-4212-aef0-ea35e85f7220",
   "metadata": {},
   "outputs": [],
   "source": [
    "tips_df['day'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cb5f65c-0e04-4552-a9bf-b5d430e2fe05",
   "metadata": {},
   "source": [
    "### Dataviz"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9cc92ac-3f03-4714-9aa9-c9f04ff93075",
   "metadata": {},
   "source": [
    "#### Basique\n",
    "\n",
    "* `df.plot()`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15b75213-4f43-43eb-8eb4-ccd74be12a21",
   "metadata": {},
   "outputs": [],
   "source": [
    "tips_df.plot();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb888e33-7d1a-43f5-aaf1-5dbdea75d754",
   "metadata": {},
   "source": [
    "* ̀`df['colonne'].hist(bins = nombre_de_barres)` permet d’avoir rapidement une idée de la distribution d’une variable quantitative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2154bbfd-ed1d-4862-8d80-5a400f7ccabf",
   "metadata": {},
   "outputs": [],
   "source": [
    "tips_df['total_bill'].hist(bins=20);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f5a5b28-5733-405f-b8c9-4d52ca039790",
   "metadata": {},
   "source": [
    "#### `seaborn`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62d0f47a-a6e4-4ba6-8504-d301b5b997fb",
   "metadata": {},
   "source": [
    "* ̀`sns.histplot()` ermet de visualiser une distribution avec en prime l’option `kde` qui permet d’avoir une estimation de la courbe de densité"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "841ea92e-81cc-4518-8c5b-001b53397acb",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot(tips_df['total_bill'], kde = True);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ca405c1-4768-4f56-a224-de2667c246b1",
   "metadata": {},
   "source": [
    "#### `plotly`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dca5770b-243a-47ea-a498-61152206f880",
   "metadata": {},
   "source": [
    "[plotly](https://plotly.com/python/) est une autre bibliothèque de dataviz qui met l’accent sur l’interactivité (elle forme avec Dash – basé sur Flask – un framework tout à fait adapté au dashboarding). Cette interactivité est intéressante pour l’exploration (possibilité de zoomer, etc.)\n",
    "Son module `plotly.express` permet de créer des figures complexes en une seule ligne de code, selon un format standardisé :\n",
    "`nom_de_la_fonction(dataframe,  x= ,  y= ,  title= ,  width= , height= )`, les dimensions étant données en pixel.\n",
    "\n",
    "[Cheat sheet Plotly](https://media.datacamp.com/legacy/image/upload/v1668605954/Marketing/Blog/Plotly_Cheat_Sheet.pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cb00598-88d7-4d7e-a0c4-5b6a070c70ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install plotly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd2e362d-6cf4-4d2a-87df-86a816b0d537",
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe70d65d-5deb-4935-ba2b-dc640d62c3f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "px.histogram(tips_df, x='total_bill', marginal='box', width=800, height=600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74ff122e-6a0b-44e1-9636-93d83485c9e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from plotly import figure_factory as ff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfe2576b-8d05-4c80-812b-aba74e1d3abc",
   "metadata": {},
   "outputs": [],
   "source": [
    "group_labels = ['total_bill']\n",
    "fig = ff.create_distplot([tips_df.total_bill], group_labels)\n",
    "fig.update_layout(width=800, height=600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27c6ce01-9e81-4af1-8aa8-fb770cc7f079",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "px.scatter(tips_df, \n",
    "           x='total_bill', \n",
    "           y='tip', \n",
    "           title='Tips x Total_bill', \n",
    "           width=600, \n",
    "           height=400)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af541b76-d028-436a-a8a7-69e9a0b11734",
   "metadata": {},
   "source": [
    "### Valeurs manquantes et aberrantes (outliers)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c27487c7-12a1-426a-93ea-f60c14da9344",
   "metadata": {},
   "source": [
    "* `df.count()` pour compter les valeurs non-manquantes (non NA) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "399cdfd1-2d7d-4cf1-a59f-c10f9a4ca812",
   "metadata": {},
   "outputs": [],
   "source": [
    "tips_df.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1a313ad-51ea-4388-9f15-3abd0aa8651d",
   "metadata": {},
   "source": [
    "* `df.isnull()` ou `df.isna()` retournent un dataframe booléen où les valeurs `None` ou `np.NaN` sont à `True`. `df.notna()` fait l’opposé. Attention, les chaînes vides `''` ou des valeurs comme `np.inf` ne comptent pas comme des `NA` : il est important de bien inspecter les données pour voir comment identifier les valeurs manquantes (codes ? – chaînes ou valeurs numériques spécifiques, par exemple il n’est pas rare d’avoir des valeurs négatives là où elles ne devraient être que positives, chaînes vides ? etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50eba08b-16fd-41c2-bf34-bd84a92df075",
   "metadata": {},
   "outputs": [],
   "source": [
    "tips_na_df = tips_df.copy()\n",
    "tips_na_df.iloc[tips_na_df.shape[0]-1,0] = np.nan # remplace par une valeur nulle\n",
    "tips_na_df.isnull()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec8ade52-9206-4f71-b4b9-e39e31540671",
   "metadata": {},
   "source": [
    "* `df.dropna()` pour les éliminer. Sinon créer une fonction sur mesure pour remplacer par des valeurs (médiane, mode, etc.) en fonction de la situation  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeacab24-1f2d-4a9c-8d9f-bc0a05f1b239",
   "metadata": {},
   "outputs": [],
   "source": [
    "tips_na_df.dropna(axis = 0) # attention à l’axe ! axis = 0 par défaut. Attention au inplace = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dc5f670-845f-46e9-a1f3-04668d7f48d8",
   "metadata": {},
   "source": [
    "* des fonctions pour récupérer les index des valeurs nulles (à modifier pour repérer des valeurs nulles autres que `None` ou `np.nan`) :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a624e4ab-72c7-4835-94f5-06aecc0dc6fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_na_index(df):\n",
    "    return np.where(df.isna())\n",
    "\n",
    "get_na_index(tips_na_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77915921-49a4-49ed-b6b9-86595b271610",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_na_values(df, idx):\n",
    "    return [df.iloc[i,j] for i,j in zip(*idx)] # on utilise l’opérateur de déballage * pour « dézipper »\n",
    "\n",
    "get_na_values(tips_na_df, get_na_index(tips_na_df))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d72659fe-354b-46df-a909-81efe844d51f",
   "metadata": {},
   "source": [
    "* note : voir [cette page](https://www.geeksforgeeks.org/zip-in-python/) si vous n’êtes pas à l’aise avec la fonction `zip()`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b5cd732-a261-4b2f-b9d7-7ddcf7342aab",
   "metadata": {},
   "source": [
    "* `px.box()` avec `plotly` pour créer des boîtes à moustache où on peut lire facilement les valeurs :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6b6d237-9f46-4803-9fed-fce62cf647b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "px.box(tips_df, y=\"total_bill\", width=600, height=400)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c252456a-26dd-43ad-a824-7f3fbc9a1adb",
   "metadata": {},
   "source": [
    "* repérer les outliers à l’aide d’une fonction sur mesure (méthode des IQR, voir le récap de la séance 3 pour d’autres méthodes : écart-type, standardisation…)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66aad883-568b-47d1-b922-77973c8d4a0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Répérer les outliers à partir des IQR\n",
    "def IQR_outliers(df_col: pd.Series) -> tuple:\n",
    "    Q1 = df_col.quantile(0.25)\n",
    "    Q3 = df_col.quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    limite_sup = Q3 + 1.5 * IQR\n",
    "    limite_inf = Q1 - 1.5 * IQR\n",
    "\n",
    "    return (limite_inf, limite_sup)\n",
    "\n",
    "IQR_outliers(tips_df.total_bill)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e312153-eaff-4645-a7a0-1ca26bfba083",
   "metadata": {},
   "source": [
    "### Tendances"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a95a214c-baef-469d-ac5c-94d46344cf22",
   "metadata": {},
   "source": [
    "* `df.corr()` une table de corrélation nous permet déjà d’émettre des hypothèses sur des relations entre variables quantitatives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f19fabf6-e425-437e-a40d-8d4b8e90bacc",
   "metadata": {},
   "outputs": [],
   "source": [
    "tips_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d110db9c-3f95-4a74-9046-a35e475f4bd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "tips_df[['total_bill', 'tip', 'size']].corr()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11941e53-aaa4-421a-92f4-1aedbb9e7e30",
   "metadata": {},
   "source": [
    "* `sns.heatmap()` nous permet de visualiser plus facilement les corrélations plt.figure(figsize=(11,7))\n",
    "sns.heatmap(orders.select_dtypes(exclude = [\"object\"]).corr(), \n",
    "            cmap='coolwarm', \n",
    "            annot = True, \n",
    "            annot_kws={\"size\": 12})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "362424e2-86fa-40fa-9a24-d975cd743097",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6,5))\n",
    "sns.heatmap(tips_df[['total_bill', 'tip', 'size']].corr(), \n",
    "            cmap='coolwarm', \n",
    "            annot = True, # affiche les valeurs dans les cellules\n",
    "            annot_kws={\"size\": 12}); # taille des chiffres dans les cellulles"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34afbd6d-6df8-4625-bdf3-0605b2593e4d",
   "metadata": {},
   "source": [
    "* `sns.pairplot()` vous permet de tracer des nuages de points en considérant des variables deux à deux (un peu comme un tableau de corrélation de la dataviz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "202800f1-70fc-4710-b5c0-b91d1b9e29e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(tips_df, height=2);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e03c8178-2971-475f-be5d-0639e6066451",
   "metadata": {},
   "source": [
    "* lorsque l’on a repéré des relations intéressantes, on peut les observer plus en détail avec :\n",
    "    * `sns.jointplot()` pour avoir une idée des distribution dans un nuage de point\n",
    "    * `sns.regplot()` pour la relation entre variable quantitative et une estimation de la tendance (droite de regression)\n",
    "    * `sns.countplot()` avec l’argument `hue=` pour mettre en relation des variables catégorielles\n",
    "    * `sns.stripplot()` ou `sns.swarmplot()` pour mettre en relation des variables quantitatives et catégorielles, en améliorant la visibilité de la dispersion\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e233b444-30df-4bcf-8a61-3c2190dd605c",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.jointplot(x='total_bill', y='tip', data=tips_df);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3103a172-4915-41b0-b0ca-5af192832aa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.regplot(x='total_bill', y='tip', data=tips_df);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f0b426e-3fde-4cbb-9e47-b487d75cef8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.countplot(x='day', data=tips_df, hue='sex');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbc54d98-079c-4248-b766-f66d6108daba",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.stripplot(x='time', y='total_bill', data=tips_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e09e190e-dbf9-405f-932d-f78e1d62b26e",
   "metadata": {},
   "source": [
    "* l’argument `trendline` dans un graphe de type `px.scatter()`, permet également d’afficher une tendance avec `plotly`, il faut alors indiquer le type d’estimation voulu. Par exemple, pour une régression un estimateur courant est `ols` (on verra cela en détail dans le cours sur la régression). Cette fonction nécessite la bibliothèque `statsmodels`. `plotly` permet alors d’afficher la valeurs des paramètres estimés et d’autres indicateurs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a117a7d-ae9c-4166-999a-caa08941f8ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install statsmodels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e3ba98d-25cb-46d4-9f23-4c3c4bb2f6af",
   "metadata": {},
   "outputs": [],
   "source": [
    "px.scatter(tips_df, x='total_bill',y='tip',trendline='ols', width=600, height=400)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f195d16-dbe8-47a6-9976-1f8d9fc2df36",
   "metadata": {},
   "source": [
    "### `ydata-profiling`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68d2058f-42ef-4e8a-b390-ea953b64cc2d",
   "metadata": {},
   "source": [
    "https://pypi.org/project/ydata-profiling/\n",
    "\n",
    "* créer un rapport d’analyse exploratoire de dataframes, généré automatiquement\n",
    "* exporter le rapport dans différents formats (HTML, JSON…)\n",
    "* **Note : ydata est très chatouilleux sur les dépendances, je conseille fortement de mettre en place un environnemnet virtuel dédié, ou d’être très attentifs sur les versions des dépendances (déjà) installées**\n",
    "\n",
    "Example : [EDA du fameux dataset « Titanic »](https://docs.profiling.ydata.ai/latest/examples/titanic/titanic_report.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b22107dd-bcc7-41c1-9674-6e9a7810ba0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!deactivate\n",
    "#!python3 -m venv ydata\n",
    "#!source ydata/bin/activate\n",
    "#!pip install ydata-profiling\n",
    "from ydata_profiling import ProfileReport\n",
    "!mkdir reports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4352af80-d905-418a-ba41-5d4900656a69",
   "metadata": {},
   "outputs": [],
   "source": [
    "profile = ProfileReport(tips_df, title = 'tips')\n",
    "profile.to_file('reports/tips_report.html')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d58ccbcf-c3ea-433e-b3e9-edaab6d5e0aa",
   "metadata": {},
   "source": [
    "## Echantillonnage"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdbaa146-b203-4266-b93e-a5b3a9c8ebd9",
   "metadata": {},
   "source": [
    "### Position du problème, échantillonnage"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb6dbded-47eb-4628-9b76-fd7d352e401e",
   "metadata": {},
   "source": [
    "![Echantillonage illustration](./images/Echantillonnage.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d8ce580-f083-4aaf-b294-dcacbb59b4ef",
   "metadata": {},
   "source": [
    "Notre objectif va être d’inférer une règle générale (sur une population) à partir d’une observation limité (sur un échantillon). \n",
    "\n",
    "On ne pourra jamais savoir si on a raison ou pas, quelle est la « vérité » sur la population. Notre stratégie va être de chercher à déterminer :\n",
    "\n",
    "* à quel point on commet une erreur en généralisant la règle\n",
    "\n",
    "* quel risque on prend en généralisant, quelle est notre marge d’erreur"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35a3f72d-e036-47af-b922-38b8c560d9c1",
   "metadata": {},
   "source": [
    "### Théorême central limite, loi des grands nombres"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cecfe089-eb38-432c-84f6-92c63bb6ef70",
   "metadata": {},
   "source": [
    "Considérons un ensemble de variables aléatoires indépendantes et uniformément distribuées. Par exemple : *n* tirages à pile ou face, ou *n* tirages d’un dé à 6 faces.\n",
    "\n",
    "Considérons les sommes de ces tirages.\n",
    "\n",
    "Par exemple, dans le cas où *n* = 6 :\n",
    "\n",
    "* à pile (0) ou face (1) :\n",
    "\n",
    "| Tirage 1      | Tirage 2      | Tirage 3      | Tirage 4      | Tirage 5      | Tirage 6      | Somme         |\n",
    "| ------------- | ------------- | ------------- | ------------- | ------------- | ------------- | ------------- |\n",
    "| 0             | 1             | 0             | 1             | 0             | 1             | 3             |\n",
    "| 1             | 0             | 0             | 1             | 1             | 1             | 4             |\n",
    "| 1             | 0             | 1             | 1             | 1             | 0             | 4             |\n",
    "| 1             | 0             | 0             | 0             | 1             | 1             | 3             |\n",
    "| 1             | 0             | 1             | 0             | 1             | 0             | 3             |\n",
    "| 1             | 0             | 1             | 1             | 0             | 1             | 4             |\n",
    "\n",
    "* lancés de dés à 6 faces :\n",
    "\n",
    "| Tirage 1      | Tirage 2      | Tirage 3      | Tirage 4      | Tirage 5      | Tirage 6      | Somme         |\n",
    "| ------------- | ------------- | ------------- | ------------- | ------------- | ------------- | ------------- |\n",
    "| 6             | 3             | 4             | 5             | 4             | 3             | 25            |\n",
    "| 3             | 4             | 4             | 5             | 3             | 5             | 24            |\n",
    "| 3             | 2             | 1             | 1             | 5             | 2             | 14            |\n",
    "| 1             | 6             | 2             | 2             | 3             | 1             | 15            |\n",
    "| 5             | 6             | 1             | 6             | 1             | 2             | 21            |\n",
    "| 3             | 4             | 5             | 1             | 4             | 1             | 18            |\n",
    "\n",
    "Le théorème central limite nous dit que la distribution des sommes obtenues va tendre vers une distribution normale, avec l’augmentation du nombre de tirages. La moyenne observée sur des échantillons tirés au hasard va se distribuer selon une loi normale centrée sur la moyenne de la population au fur et à mesure que l’on augmente la taille des échantillons.\n",
    "\n",
    "Considérons la suite suivante de 6 tirages à pile ou face dont on a caclulé les sommes :\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d95c1fa-3250-473f-9309-9c3b659a824c",
   "metadata": {},
   "outputs": [],
   "source": [
    "pileface = [3, 4, 4, 3, 3, 4, 4, 4, 5, 4, 5, 5, 4, 2, 4, 3, 4, 4, 2, 3, 1, 4, 5, 3, 2, 2, 5, 2, 4, 4]\n",
    "pd.DataFrame(data=pileface).describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a64df546-b81f-43af-a816-4ea0caf16f00",
   "metadata": {},
   "source": [
    "et pour les dés :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c3c9c13-7dc8-4cb0-a887-8f3f45d8d9f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "des6 = [25, 24, 14, 15, 21, 18, 18, 26, 19, 20, \n",
    "       23, 17, 16, 20, 14, 23, 24, 20, 26, 31, \n",
    "       27, 20, 15, 25, 22, 24, 25, 20, 19, 22, \n",
    "       26, 16, 27, 21, 22, 16, 21, 13, 18, 12,\n",
    "       20, 23, 27, 22, 16, 14, 18, 16, 19, 20,\n",
    "       30, 21, 24, 20, 23, 26, 22, 21, 22, 24,\n",
    "       22, 23, 22, 17, 17, 17, 26, 25, 20, 20,\n",
    "       28, 12, 15, 17, 18, 20, 22, 29, 19, 20,\n",
    "       24, 18, 15, 20, 19, 24, 20, 17, 25, 15,\n",
    "       23, 21, 27, 22, 19, 21, 20, 22, 16, 17,\n",
    "       17, 17, 18, 25, 23, 20, 24, 20, 24, 18,\n",
    "       27, 20, 23, 29, 22, 21, 24, 16, 26, 18,\n",
    "       20, 27, 18, 13, 22, 24, 15, 33, 20, 15,\n",
    "       20, 16, 23, 22, 25, 14, 24, 27, 26, 18,\n",
    "       24, 24, 19, 20, 23, 24, 16, 20, 18, 12,\n",
    "       19, 17, 30, 21, 28, 17, 22, 21, 19, 33,\n",
    "       24, 19, 23, 19, 25, 20, 19, 17, 16, 20,\n",
    "       21, 17, 26, 14, 32, 17, 25, 31, 22, 26,\n",
    "       20, 18, 19, 16, 19, 23, 22, 24, 21, 23,\n",
    "       15, 20, 27, 17, 20, 25, 14, 17, 28, 20]\n",
    "len(des6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "516a0207-ed70-49c4-b57b-d475b17d9c5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "pileface_data = [pileface[:10], pileface[:20], pileface]\n",
    "des6_data = [des6[:10], des6[:50], des6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1108f99-0abf-4647-985b-b4c9331426b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "group_labels = ['10 lancers', '50 lancers', '200 lancers']\n",
    "colors = ['#A56CC1', '#6666EC', '#63F5EF']\n",
    "\n",
    "fig = ff.create_distplot(des6_data, group_labels, colors=colors,\n",
    "                         bin_size=.2, show_rug=False)\n",
    "\n",
    "# Mise en forme de la figure\n",
    "fig.update_layout(title_text='Théorème central limite : approche empirique avec tirages de 6 dés', \n",
    "                  width=800, \n",
    "                  height=600)\n",
    "fig.update_xaxes(range=[6, 36])\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43fad8ca-9a4b-4c5f-8216-9d7825afd299",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(data=des6).describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "738f2175-1f34-458f-86b5-f3a706c20b4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "des100 = [378, 327, 348, 324, 350, 372, 365, 358, 336, 343,\n",
    "          341, 339, 365, 337, 351, 366, 304, 363, 367, 365,\n",
    "          352, 339, 344, 333, 365, 348, 371, 352, 336, 333,\n",
    "          357, 380, 348, 357, 356, 347, 385, 343, 350, 348,\n",
    "          337, 343, 362, 377, 342, 306, 363, 363, 356, 336,\n",
    "          314, 375, 342, 327, 343, 343, 349, 351, 357, 337,\n",
    "          358, 324, 369, 328, 359, 373, 358, 365, 367, 333,\n",
    "          384, 370, 349, 327, 330, 367, 379, 371, 345, 366,\n",
    "          315, 377, 365, 348, 367, 301, 338, 320, 365, 355,\n",
    "          358, 331, 352, 367, 353, 346, 376, 342, 342, 345,\n",
    "          347, 365, 345, 335, 361, 364, 360, 366, 328, 356,\n",
    "          387, 329, 338, 357, 312, 326, 361, 296, 343, 368,\n",
    "          356, 359, 338, 358, 335, 366, 365, 351, 364, 369,\n",
    "          344, 342, 342, 349, 370, 338, 350, 377, 362, 339,\n",
    "          338, 355, 364, 374, 336, 362, 336, 351, 372, 365,\n",
    "          361, 310, 334, 336, 364, 367, 358, 327, 358, 347,\n",
    "          316, 356, 347, 345, 347, 371, 333, 346, 400, 365,\n",
    "          335, 340, 374, 353, 340, 345, 367, 357, 362, 348,\n",
    "          336, 335, 336, 333, 334, 379, 324, 359, 370, 323,\n",
    "          356, 356, 337, 338, 338, 361, 319, 333, 340, 359,\n",
    "          360, 356, 340, 330, 370, 351, 365, 322, 363, 357,\n",
    "          358, 357, 371, 348, 322, 355, 337, 373, 332, 335,\n",
    "          368, 340, 346, 361, 347, 354, 344, 358, 350, 349,\n",
    "          387, 367, 322, 336, 349, 346, 359, 327, 343, 364,\n",
    "          335, 321, 344, 330, 329, 378, 322, 359, 359, 348,\n",
    "          376, 362, 358, 338, 344, 380, 356, 359, 361, 360,\n",
    "          364, 367, 333, 357, 336, 321, 353, 343, 334, 354,\n",
    "          364, 332, 363, 342, 359, 348, 364, 338, 349, 354,\n",
    "          345, 346, 350, 335, 339, 356, 353, 352, 332, 315,\n",
    "          344, 349, 334, 341, 355, 345, 349, 337, 349, 327]\n",
    "len(des100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed46a38e-13e2-495e-8619-7b748b194a0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "des100_data = [des100[:10], des100[:50], des100[:100], des100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18b87215-fc11-47af-8831-278f59bfe5a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "group_labels = ['10 lancers', '50 lancers', '100 lancers', '300 lancers']\n",
    "colors = ['#A56CC1', '#6666EC', '#63F5EF', '#22AF66']\n",
    "\n",
    "fig = ff.create_distplot(des100_data, group_labels, colors=colors,\n",
    "                         bin_size=.5, show_rug=False)\n",
    "\n",
    "# Add title\n",
    "fig.update_layout(title_text='Théorème central limite : approche empirique avec tirages de 100 dés', \n",
    "                  width=800, \n",
    "                  height=600)\n",
    "\n",
    "fig.update_xaxes(range=[100, 600])\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2ce623c-780a-43e4-9516-2a7e8d8958d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(data=des100).describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac51fbb0-40fb-4b7a-81da-802d394e8563",
   "metadata": {},
   "source": [
    "Le théorème de la limite centrale s’applique pour tout type de distribution de probabilité. Nous l’avons présenté pour des événements obéissants à des lois uniformes (lancers de pièce de monnaie ou de dés à 6 faces), mais il est valable également pour des distributions bimodales :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65e072cd-a595-41a0-9c74-7f29d7cff075",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Générer une distribution bimodale asymétrique\n",
    "np.random.seed(42)\n",
    "data1 = np.random.normal(loc=2, scale=0.5, size=5000)\n",
    "data2 = np.random.normal(loc=6, scale=1.5, size=5000)\n",
    "data = np.concatenate([data1, data2])\n",
    "\n",
    "# Histogrammes distribution initiale\n",
    "sns.histplot(data, kde = True).set_title('Distribution initiale - bimodale assymétrique');\n",
    "\n",
    "# Tirer un échantillon et calculer la moyenne\n",
    "def sample_means(data, sample_size, num_samples):\n",
    "    means = [np.mean(np.random.choice(data, size=sample_size)) for _ in range(num_samples)]\n",
    "    return means\n",
    "\n",
    "# Taille des échantillons à tester\n",
    "sample_sizes = [5, 20, 100]\n",
    "num_samples = 1000\n",
    "\n",
    "# Histogrammes des moyennes des échantillons\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "for i, sample_size in enumerate(sample_sizes):\n",
    "    means = sample_means(data, sample_size, num_samples)\n",
    "    g = sns.histplot(ax=axes[i], data=means, kde=True)\n",
    "    g.set_title(f'Sample size = {sample_size}')\n",
    "    g.set_xlim(left=0, right=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "523d8920-3c5e-4fc5-adb8-e0a510ef010c",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(data).describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63359007-68c0-42d8-ba8d-c27dc7ed9008",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import expon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df3ee35f-6328-42ce-80b4-3e61a229e58b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import expon\n",
    "\n",
    "# Générer une distribution exponentielle\n",
    "np.random.seed(42)\n",
    "data = expon.rvs(scale=1, size=10000)\n",
    "\n",
    "# Histogrammes distribution initiale\n",
    "sns.histplot(data, kde = True).set_title('Distribution initiale - exponentielle');\n",
    "\n",
    "# Fonction pour tirer un échantillon et calculer la moyenne\n",
    "def sample_means(data, sample_size, num_samples):\n",
    "    means = [np.mean(np.random.choice(data, size=sample_size)) for _ in range(num_samples)]\n",
    "    return means\n",
    "\n",
    "# Taille des échantillons à tester\n",
    "sample_sizes = [5, 20, 100]\n",
    "num_samples = 1000\n",
    "\n",
    "# Histogrammes des moyennes des échantillons\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "for i, sample_size in enumerate(sample_sizes):\n",
    "    means = sample_means(data, sample_size, num_samples)\n",
    "    g = sns.histplot(ax=axes[i], data=means, kde=True)\n",
    "    g.set_title(f'Sample size = {sample_size}')\n",
    "    g.set_xlim(left=0, right=8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ad69d62-de18-42cf-bb56-33f26c3391c7",
   "metadata": {},
   "source": [
    "### TCL et échantillonage\n",
    "\n",
    "* individus caractérisés par des observables (p. ex. : taille, poids, revenus, niveau d’étude…)\n",
    "* population dans laquelle chaque observable a une moyenne **μ** et une dispersion (e.-t.) **σ**, selon une distribution (souvent normale, mais pas forcément)\n",
    "* on tire de cette population des échantillons de taille n où l’observable a une moyenne **x̄** et une dispersion **s**\n",
    "* ces moyennes calculées à partir des échantillons sont distribuées selon une distribution normale \n",
    "* plus la taille de l’échantillon est importante et plus :\n",
    "    * **x̄** sera proche de **μ** (meilleur estimateur), et la moyenne **μ(x̄)** des moyennes sera également égale à *μ*\n",
    "    * la distribution, de dispersion **σ(x̄)** sera ressérée autour de cette moyenne (limitée par la croissance de la fonction racine de n : la croissance de l’échantillon aura un effet limité au bout d’un moment – cf. les échantillons dans les sondages)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bc137b8-9865-4bd5-82d1-11a7ebfbdc66",
   "metadata": {},
   "source": [
    "![TCL et échantillonage](./images/TCLetEchantillon.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d7bde6c-0c2a-406f-8b36-29f78774a3db",
   "metadata": {},
   "source": [
    "### Intervalles de confiance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f7abed0-a223-4994-822d-8a44e3e45116",
   "metadata": {},
   "source": [
    "De tout ce qui a été vu précédemment, on peut tirer des conclusions en prenant le problème à l’envers : si je prends un échantillon unique de taille n tiré d’une population, quelle information sur la population puis-je en tirer ? et surtout, notre principal problème va être de déterminer à quel point on se trompe dans notre estimation des caractéristiques de la population à partir des observations sur l’échantillon. C’est le rôle des intervalles de confiance.\n",
    "\n",
    "![Intervalle confiance et distribution](./images/IntervalleConfiance.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bdfc97c-d9b8-4389-be60-ead5588d6898",
   "metadata": {},
   "source": [
    "* Imaginons une population dont nous cherchons à déterminer la taille moyenne *µ*.\n",
    "* Nous sélectionnons un échantillon de 1000 personnes, où nous mesurons une moyenne *x̄* de 180cm et 20cm d’écart-type *s* (dans l’échantillon).\n",
    "* Nous savons donc que la moyenne que nous observons est tirée d’une distribution centrée sur *μ* et de dispersion : $$\\frac{\\sigma}{\\sqrt{n}} \\thickapprox \\frac{s}{\\sqrt{n}} = \\frac{20}{\\sqrt{1000}} = 0.63$$\n",
    "* Or *x̄* est le meilleur estimateur (*maximum likelihood estimate* ou LME) de *μ*. Donc on *estime* que *μ* = 180cm. À quel point dit-on une bêtise en choisissant cette valeur ?\n",
    "* Nous avons 67% de chance que d’être tombé à moins d’1 écart-type (de la distribution) *μ* avec notre échantillon. Nous avons avons 95% de chance d’être tombé à 2 écart-type de *μ*.\n",
    "* Ainsi on peut écrire qu’avec un intervalle de confiance de 95%, *µ* = 180cm ± 0,63 x 2 = 180cm ± 1,26 = \\[178,74 ; 181,26]\n",
    "* Notes :\n",
    "    * il faut que les tirages soient indépendants, idéalement il faudrait des tirages avec remise. Si l’échantillon est suffisamment petit par rapport à la taille de la population (en général < 10%), on peut considérer que la chance de tirer deux fois la même observation avec remise est si faible que ça ne fait pas de différence qu’il y ait eu remise ou pas\n",
    "    * nous avons eu besoin de réaliser une approximation qui n’est valable que si *n* est assez grand, de même pour que le TCL soit valide. Empiriquement on considère que c’est le cas dès que *n* > 30\n",
    "    * si *n* est plus petit que 30 (mais supérieur à 10) alors il faut vérifier qu’on n’a pas d’outlier et que l’on peut supposer que les distributions ne sont pas dissymétriques (*skewness*)\n",
    "    * si on est sûr que l’obsevable est distribuée selon une loi normale, alors la taille *n* importe peu\n",
    "    * la logique derrière ce raisonnement est centrée sur l’erreur et non la valeur réelle de la moyenne recherchée : à quel point j’ai « pas de bol » d’avoir sélectionné – au hasard – un échantillon composé d’observables qui étaient la plupart très éloignées de la valeur moyenne dans la population ? **À aucun moment un intervalle de confiance ne nous dit que la moyenne est de 180cm à 95%** par exemple.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f92ce09-0df9-40f5-9e03-f0526313e73a",
   "metadata": {},
   "source": [
    "#### Code pour déterminer les intervalles de confiance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fd0c466-a563-4573-b536-73889faf67b7",
   "metadata": {},
   "source": [
    "Nous allons utiliser deux méthodes du module `stats` de la bibliothèque `scipy` : `.cdf()` (*cumulative distribution function*) ou fonction de répartition qui retourne la la probabilité qu'une variable aléatoire prenne une valeur inférieure ou égale à un réél donné (en argument). La fonction inverse, , est `.ppf`(*percent point function*) ou fonction quantile. Cette dernière nous permettra, pour un intervalle de confiance donné, de retrouver les valeurs (sup et inf) correspondantes dans la distribution, et la première nous indiquera quel intervalle correspond à des valeurs sup et inf données."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d3ae3a8-8721-4ba0-8d0d-280e031f3d4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fb8417f-d402-40f2-93ea-09f329cad721",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Génération de la distribution d’échantillonage\n",
    "distri_ech = stats.norm(180, 0.63)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51672b09-068d-4c82-ba80-c9e873ddafc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "intervalle_confiance = 0.95\n",
    "\n",
    "# on détermine les limites sup et inf\n",
    "proba_sup = 0.5 + intervalle_confiance/2\n",
    "proba_inf = 0.5 - intervalle_confiance/2\n",
    "\n",
    "mu_sup = distri_ech.ppf(proba_sup)\n",
    "mu_inf = distri_ech.ppf(proba_inf)\n",
    "\n",
    "print(f'Intervalle de confiance à {intervalle_confiance}% = [{mu_inf} ; {mu_sup}]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59b04165-b90c-4dc5-99ee-047699993f42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# on détermine l’intervalle de confiance\n",
    "mu_inf = 178.74\n",
    "mu_sup = 181.26\n",
    "intervalle_confiance = distri_ech.cdf(mu_sup) - distri_ech.cdf(mu_inf)\n",
    "print(f'L’intervalle de confiance de l’intervalle [{mu_inf} ; {mu_sup}] est {intervalle_confiance}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca2dc041-6e67-42c4-bd1c-a9f2b02ee792",
   "metadata": {},
   "source": [
    "## Test d’hypothèse"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11980e8a-4614-49d8-bab7-fc5dc241a75d",
   "metadata": {},
   "source": [
    "### L’hypothèse nulle : un raisonnement par l’absurde"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98900589-90ae-43cf-9ef7-4c44098faa8a",
   "metadata": {},
   "source": [
    "En général, on est intéressé de comparer des moyennes quand on compare des groupes ou des catégories différentes, par exemple la taille moyenne des hommes et celle des femmes.\n",
    "Une utilisation notable est lors que l’on réalise des expériences (médicales, recherche, etc.) ou alors en marketing : on veut comparer l’effet d’une nouvelle caractéristique d’un produit sur le comportement ou le jugement de groupes de personnes différentes. On appelle cela les test A/B (où on compare un groupe A et un groupe B). Par exemple on veut savoir si un changement de conception dans un moteur limite son émission de CO2. On va avoir deux catégories : ancienne conception et nouvelle conception, et on veut savoir si les émissions moyennes de CO2 sont différentes entre les deux conceptions.\n",
    "\n",
    "On va appliquer un raisonnement par l’absurde.\n",
    "\n",
    "* Supposons qu’il n‘y a pas de différence entre les deux groupes. On fait l’hypothèse que le changement de conception n’a aucun effet. C’est ce que l’on appelle *l’hypohèse nulle*, noté **H0**\n",
    "* Nous allons constituer deux groupes (*ancienne génération* et *nouvelle génération*), en choisissant (au hasard) des moteurs (n=100 par exemple) de chaque type au hasard parmi les moteurs produits dans les usines\n",
    "* On mesure les émissions de CO2 ans chaque groupe. Supposons que *ancien* = 3000g CO2/heure avec une dispersion de 500 grammes et *nouveaux* = 2900kg CO2/heure. Les nouveaux semble moins émettre, mais cela est-t-il statistiquement *significatif* ?\n",
    "* La logique va être de se demander : si les groupes ne sont pas différents, quelle est la probabilité que j’observe, en sélectionnant au hasard, des moteurs qui délivrent 2,8kg ou moins ? Si cette probabilité est inférieure à un certain seuil, que je vais fixer d’avance, alors je vais considérer que cette probabilité est trop faible pour que mon hypothèse H0 soit valide : je vais la rejeter (ce qui est différent de dire qu’elle est vraie). Dans le cas inverse, je ne vais pas pouvoir la rejeter. On ne saura jamais quelle est la réalité, aussi selon le choix que l’on fait, on peut commettre deux types d’erreurs différentes (cf. tableau ci-dessous)\n",
    "* On appelle le seuil **α** et la probabilité d’observer une valeur en supposant H0, la **p-value**\n",
    "\n",
    "|  Réalité       | H0 non-rejetée (p>α) | H0 rejetée (p<α) |\n",
    "|----------------|----------------------|------------------|\n",
    "| H0 est vraie   |          OK          | Erreur de type I |\n",
    "| H0 est fausse  | Erreur de type II    |        OK        |\n",
    "\n",
    "* Erreur de type I = faux positifs\n",
    "* Erreur de type II = faux négatifs\n",
    "* En décalant le seuil (plus ou moins de tolérance) on va augmenter tel ou tel type d’erreur : il va falloir arbitrer !"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "119695d8-244c-4ae7-9fc5-cc3be26e7b6f",
   "metadata": {},
   "source": [
    "### Coder un test d’hypothèse\n",
    "\n",
    "1. Créer la distribution d’échantillonage (méthode `norm()` dans `scipy.stats`) avec les paramètres du problème (moyenne, dispersion)\n",
    "2. Calculer la *p-value* avec la méthode `.cdf()` pour la valeur moyenne testée (celle du groupe que l’on évalue). Attention, ce qui nous intéresse c’est la valeur au-delà du point\n",
    "3. Comparer la valeur de *p* avec celle du seuil *α* qu’on s’est fixé a priori, et rejeter ou non l’hypothèse nulle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9f6f6d8-f0f1-4a5d-b048-6c652fc4a08b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import norm\n",
    "\n",
    "alpha = 0.05\n",
    "mean_tested = 2900\n",
    "n = norm(3000, 500/(100**.5))\n",
    "\n",
    "p = n.cdf(mean_tested)\n",
    "\n",
    "if alpha/2 >= p:\n",
    "    print(f'L’hypothèse H0 est rejetée avec p={p} et α={alpha}')\n",
    "else:\n",
    "    print(f'L’hypothèse H0 ne peut pas être rejetée avec p={p} et α={alpha}')\n",
    "\n",
    "x = np.linspace(2500, 3500, 10000)\n",
    "pdf = n.pdf(x)\n",
    "fig, ax = plt.subplots(figsize=(8, 5))\n",
    "ax.plot(x, pdf)\n",
    "ax.set_title(\"Distribution d’échantillonnage pour H0\");\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55960057-ab5f-4aa4-b2f5-57ec51c88c5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(8, 5))\n",
    "ax.set_title(\"Mise en évidence de la p-value\");\n",
    "ax.set_xlim(2800, 3000) # on se positionne sur le bon intervalle\n",
    "ax.set_ylim(0, 0.01)\n",
    "i = x <= mean_tested # boolean indexing pour colorer la zone qui nous intéresse\n",
    "ax.fill_between(x[i], y1=0, y2=pdf[i], color='red') # on colore entre l’axe et la courbe pour les index sélectionnés précédemment\n",
    "props = dict(facecolor='black', width=1, headwidth=5, headlength=8) # on définit les propriétés de la flèche\n",
    "ax.annotate(f'p={round(p,5)}', (2885, 7e-4), (2825, 0.004), arrowprops=props) # on affiche p-value et flèche\n",
    "ax.plot(x, pdf, color='blue');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "058247cb-3341-4e3d-b6fc-9e090d80c12e",
   "metadata": {},
   "source": [
    "Il est d’usage d’apposer des astérisques selon le niveau de significativité de la p-value pour faciliter la lecture : * pour p < 0,05 ; ** pour p < 0,01, *** pour p < 0,001 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96e47ab3-9b27-402a-bf46-a415ccaf1744",
   "metadata": {},
   "source": [
    "### Exercice : \n",
    "1. Modifiez quelques paramètres (taille de l’échantillon, moyenne du groupe test, dispersion…) pour voir comment réagit le test\n",
    "2. Il y a un bug dans ce code de test de H0. Saurez-vous le mettre en évidence et le corriger ?\n",
    "3. Bonus : modifiez le code pour agrémenter la p-value d’astérisques selon le niveau de significativité "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48b2f514-9af4-462e-be61-edfb093964c2",
   "metadata": {},
   "source": [
    "### Le test-t et la distribution de Student"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd4efedf-ce82-48be-8052-3cfef58d2316",
   "metadata": {},
   "source": [
    "Dans ce que nous avons vu ci-dessus, beaucoup repose sur le fait que nous avons pu estimer la dispersion de la distribution d’échantillonage à partir de la dispersion dans l’échantillon observé. En effet, la formule : $$\\frac{\\sigma}{\\sqrt{n}} \\thickapprox \\frac{s}{\\sqrt{n}}$$ n’est valable que si *n* est assez grand.\n",
    "\n",
    "On pourrait chercher à contourner le problème par une standardisation des moyennes, et dans ce cas on peut se contenter d’une loi normale centrée réduite, que l’on connaît bien. On peut la calculer ainsi pour la distribution d’échantillonage : $$\\frac{\\bar{X} - \\mu}{\\sigma / \\sqrt{n}} = \\frac{\\bar{X} - \\mu(\\bar{X})}{s / \\sqrt{n}}$$ Hélas cela ne fonctionne pas. En effet dans ce cas aussi on a besoin de pouvoir approcher *σ* à l’aide de *s*. Malheureusement cela n’est possible que si *n* est suffisamment grand, c’eset le même problème que tout à l’heure.\n",
    "\n",
    "Le problème vient de ce que quand *n* est petit, on a de trop fortes chances d’obtenir des points éloignés de la tendance centrale, qui aurait alors trop de poids dans nos estimations par rapport à une loi normale. Pour résoudre le problème posé par les petits échantillons, Student (pseudonyme de [William Gosset](https://fr.wikipedia.org/wiki/William_Gosset)) a proposé une distribution dépendante de la taille de l’échantillon (elle a *n-1* degrés de liberté) :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69f948f3-fb09-4f36-8fda-eaa1c59b41b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import t\n",
    "\n",
    "degres_liberte = [1, 2, 5, 30]\n",
    "\n",
    "x = np.linspace(-5, 5, 1000)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "for ddl in degres_liberte:\n",
    "    y = t.pdf(x, ddl)\n",
    "    g = sns.lineplot(x=x, y=y, label=f'ν = {ddl}')\n",
    "    g.set_title('Distribution de Student')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daabd8ed-e78c-4acd-8365-0c575e298194",
   "metadata": {},
   "source": [
    "Comparée à une distribution normale, elle en est très proche, avec une base d’autant plus épaisse (*skewness*) que les degrés de libertés sont petits, ce qui reflète la possibilité d’observer une dispersion plus importante pour les petits échantillons (on autorise plus d’évènements « extrêmes » dans les petits échantillons). Plus *n* augmente et plus elle tend vers une distribution normale."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cedb5ce-46f7-48ef-86a0-ab2d4cd315ce",
   "metadata": {},
   "source": [
    "### Exercice (2) :\n",
    "\n",
    "1. Coder un test d’hypothèse avec la distribution de Student. Rappel : (1) créer la distribution avec les paramètres du problème, (2) calculer la p-value (3) conclure. Attention, rédiger une phrase de conclusion qui interprête correctement les résultats.\n",
    "2. Rechercher dans la doc de scipy si des méthodes permettent de faire directement les différents types de tests, calculer les intervalles de confiance, etc.\n",
    "3. Calculer des distributions d’échantillonages, des intervalles de confiance, et faire des comparaisons de moyennes (test) avec les données que vous avez sélectionnées et explorées pour aujourd’hui. Bien rédiger les interprétations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2260e25-ce4e-410e-b910-c5729d6c317c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
