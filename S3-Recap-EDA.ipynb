{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e44744c5-18b5-4fc9-8f34-74e67145d3ed",
   "metadata": {},
   "source": [
    "# Analyse et Exploration des données S3 : Recap et approfondissement\n",
    "\n",
    "- Année 2025/2026\n",
    "- Jean Delpech\n",
    "- Classe : B3 IA/Data (Campus Aix-en-Provence)\n",
    "- Dernière mise à jour : janvier 2026"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f389e58-991c-4075-af71-51f00cddd053",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "072cb7aa-0977-4ce1-b39e-72fa34f413c4",
   "metadata": {},
   "source": [
    "## Processus analyse de données / création modèle ML"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3c351dc-f13a-4d6a-ac19-f3761c311be2",
   "metadata": {},
   "source": [
    "1. Récupérer les données brutes (.csv, API/JSON, BDD/SQL,…)\n",
    "2. Parcourir les données, prendre connaissance des colonnes/variables et **réaliser une copie de travail**\n",
    "3. Statistiques descriptives : moyenne, médiane, max, min, écart-type, quantiles, variance, covariances, corrélation…\n",
    "4. Nettoyage : gérer les ```NaN``` (suppression ? remplacement par moyenne, médiane ?), convertir les autres valeurs problématiques comme ```inf``` et ```-inf```, supprimmer des colonnes (inutiles, non exploitables)\n",
    "5. Si besoin, assembler les données (merge), créer les métriques qui nous manquent (créer de nouvelles colonnes, transformer…)\n",
    "6. Visualiser les données (distribution, mise en relation) avec des graphes adaptés au types de données (bar plots, scatter plots, cat plots, box plots…)\n",
    "7. Repérer et gérer les outliers (supprimer ou transformer)\n",
    "8. Choisir, entraîner et tester des modèles de ML ou analyse (inférence statistique)\n",
    "9. Amélioration des modèles (fine-tuning, feature selection) et comparaison des modèles les mieux ajustés (A/B testing)\n",
    "10. Déploiement"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6c65e89-992b-4d07-bbc1-e01216c41103",
   "metadata": {},
   "source": [
    "## Récap : stats descriptives\n",
    "\n",
    "Les statistiques descriptives permettent de **résumer et caractériser un nuage de points** par quelques valeurs numériques clés. Elles nous renseignent sur la **position**, la **dispersion** et les **relations** entre variables (tendances).\n",
    "\n",
    "### Mesures de tendance centrale\n",
    "\n",
    "Ces mesures nous indiquent où se situe le \"centre\" du nuage de points :\n",
    "\n",
    "**Moyenne arithmétique** : $\\bar{x} = \\frac{1}{n}\\sum_{i=1}^{n} x_i$\n",
    "\n",
    "La moyenne représente le centre de gravité du nuage de points. Elle est sensible aux valeurs extrêmes.\n",
    "\n",
    "**Médiane** : valeur qui sépare l'échantillon en deux parties égales (50% des observations en dessous, 50% au-dessus)\n",
    "\n",
    "La médiane est plus robuste aux outliers que la moyenne. Elle indique le \"milieu\" du nuage selon l'axe considéré.\n",
    "\n",
    "### Mesures d'étendue et de dispersion\n",
    "\n",
    "Ces mesures quantifient l'**étalement** du nuage de points :\n",
    "\n",
    "**Minimum et Maximum** : valeurs extrêmes de l'échantillon\n",
    "\n",
    "Ils définissent la boîte englobante du nuage de points sur chaque axe.\n",
    "\n",
    "**Variance** : $\\sigma^2 = \\frac{1}{n}\\sum_{i=1}^{n} (x_i - \\bar{x})^2$\n",
    "\n",
    "La variance mesure la dispersion quadratique moyenne autour de la moyenne. Plus elle est élevée, plus le nuage est étalé.\n",
    "\n",
    "**Écart-type** : $\\sigma = \\sqrt{\\sigma^2}$\n",
    "\n",
    "L'écart-type s'exprime dans la même unité que les données. Il représente la \"largeur\" typique du nuage autour de son centre.\n",
    "\n",
    "**Quantiles** : $Q_p$ est la valeur en dessous de laquelle se trouvent $p\\%$ des observations\n",
    "\n",
    "Les quartiles ($Q_{0.25}$, $Q_{0.5}$, $Q_{0.75}$) permettent de diviser le nuage en quatre parties égales et de visualiser sa distribution via les boîtes à moustaches (*boxplot*, voir plus bas, ou le cours sur la dataviz).\n",
    "\n",
    "### Mesures de relations entre variables\n",
    "\n",
    "Pour un nuage de points bivariés (deux variables $X$ et $Y$), ces mesures caractérisent la **forme** et l'**orientation** du nuage :\n",
    "\n",
    "**Covariance** : $\\text{Cov}(X,Y) = \\frac{1}{n}\\sum_{i=1}^{n} (x_i - \\bar{x})(y_i - \\bar{y})$\n",
    "\n",
    "La covariance indique si le nuage s'étire selon une diagonale montante (covariance positive) ou descendante (covariance négative). Une covariance proche de zéro suggère un nuage sans orientation particulière.\n",
    "\n",
    "**Coefficient de corrélation de Pearson** : $r = \\frac{\\text{Cov}(X,Y)}{\\sigma_X \\sigma_Y}$\n",
    "\n",
    "Le coefficient $r \\in [-1, 1]$ mesure l'intensité de la relation linéaire entre $X$ et $Y$. Un nuage parfaitement aligné selon une droite montante donne $r = 1$, descendante donne $r = -1$, et un nuage sphérique sans relation linéaire donne $r \\approx 0$.\n",
    "\n",
    "\n",
    "Toutes ces statistiques transforment un nuage de points complexe en quelques nombres qui décrivent sa position (moyenne, médiane), son étalement (variance, écart-type, quantiles) et sa forme générale (covariance, corrélation).\n",
    "\n",
    "Attention, ces statistiques résument des positions/relations complexes en quelques chiffres, plusieurs nuages montrant une structure parfois très différentes peuvent être résumés par les mêmes statistiques. Toujours prendre du recul et conserver son esprit critique."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d237cf3-14bb-4879-bedf-f63f8d7379c6",
   "metadata": {},
   "source": [
    "## Récap : outliers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04db817f-d65f-4822-819e-d5b3b236a069",
   "metadata": {},
   "source": [
    "### Outliers : definition\n",
    "\n",
    "[Article « Donnée aberrantes sur Wikipédia](https://fr.wikipedia.org/wiki/Donn%C3%A9e_aberrante) :\n",
    "\n",
    "> « En statistique, une donnée aberrante (anglais outlier) est une valeur ou une observation qui est « distante » des autres observations effectuées sur le même phénomène, c'est-à-dire qu'elle contraste grandement avec les valeurs « normalement » mesurées. Une donnée aberrante peut être due à la variabilité inhérente au phénomène observé, ou indiquer une erreur expérimentale. Dans ce dernier cas, elles sont parfois écartées.\n",
    "\n",
    "> […]\n",
    "\n",
    "> Une interprétation statistique naïve d'une série de données contenant des données aberrantes peut être trompeuse et induire en erreur. Par exemple, si une personne décide de calculer la température moyenne de 10 objets dans une pièce, et que 9 d'entre eux ont une température située entre 20 et 25 degrés Celsius mais que le dernier est un four en marche à 175 °C, la médiane de la série sera située entre 20 et 25 °C mais la température moyenne sera entre 35,5 et 40 °C. Dans ce cas, la médiane est un meilleur indicateur de la température des objets que la moyenne. »\n",
    "\n",
    "![Normal distribution with standard deviation](./images/Empirical_Rule.PNG)\n",
    "\n",
    "By Dan Kernler - Own work, CC BY-SA 4.0, https://commons.wikimedia.org/w/index.php?curid=36506025\n",
    "\n",
    "Dans le cas du four donné en exemple ci-dessus, la valeur aberrante est dû au fait qu’elle est produite par une autre « règle » ou un autre « phénomène » que celui qu’on recherche (four allumé vs. chaleur ambiante). C’est ce que l’on pourrait considérer comme une erreur de mesure. \n",
    "Une autre catégorie d’outlier sont des valeurs « naturelles », mais très rares (un salaire de 100 k€ par mois, ou un bébé de 7kg à la naissance, ou encore un homme d’une taille de 2,4m – penser à tous les records).\n",
    "\n",
    "Les outliers peuvent avoir de nombreux effets négatifs :\n",
    "\n",
    "- influencer négativement l’estimation des distributions et autres patterns sous-jacent aux données (distribution avec une queue plus large que la réalité, moyennes « décalées », etc.)\n",
    "- variance observée (et erreur) plus importante\n",
    "- certains modèles (comme la régression traditionnelle) sont très sensibles aux outliers, l’estimation devient moins fiable / stable\n",
    "- des modèles de ML peuvent faire du surapprentissage sur des données aberrantes, les identifiant comme des données singulièrement importantes et de nouvelles données sans outliers n’étant pas traitées correctement\n",
    "\n",
    "Les outliers peuvent être de différents types :\n",
    "- ne concerner qu’une seule variable (dû à une erreur de mesure systématique ou exceptionnelle)\n",
    "- des combinaisons (patterns) de variables assez rare, mais pour lequel on n’arrive pas à capturer l’élément explicatif (par exemple faible niveau d’étude mais hauts revenus : héritage, footballeur – haut niveau de formation mais pas au sens classique, etc.)\n",
    "- des mesures effectuées dans des contextes particuliers : on voit une concentration d’outliers pour une location, ou une date donnée qui tranche avec le reste des observation, ici aussi il nous manque un évènement qui a dû survenir pour expliquer le pattern \n",
    "\n",
    "La question est : que fait-on des outliers ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49ae2053-d8f0-40a6-8f61-8a624affffee",
   "metadata": {},
   "source": [
    "### Repérer les outliers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff804e6d-6cbe-461c-bfa9-9dea631aaa6e",
   "metadata": {},
   "source": [
    "Une première chose à faire est de les repérer. La boîte à moustache nous permet d’en apprécier visuellement le nombre et la distribution (sont ils rares ? quand il y en a sont-ils très loin de la limite ?). Pour les traiter, nous devons construire des fonctions qui nous permettront de les repérer et de les trier dans nos données. On peut envisager différentes approches :\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acfb8ae8-6a30-4ffc-bfe4-06a39e0f0e8d",
   "metadata": {},
   "source": [
    "#### Repérer les outliers à l’aide des IQR"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a751c7c9-7a74-49c7-bff4-39a143c446fd",
   "metadata": {},
   "source": [
    "C’est une approche assez universelle, qui fonctionne même pour des distributions légèrement dissymétriques (« à queue épaisse » / *skewed*), et correspond à ce que montre les boîtes à moustache :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85bafa02-e70d-47c9-ae03-ce8d59ab87f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Répérer les outliers à partir des IQR\n",
    "def IQR_outliers(df_col: pd.Series) -> tuple:\n",
    "    Q1 = df_col.quantile(0.25)\n",
    "    Q3 = df_col.quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    limite_sup = Q3 + 1.5 * IQR\n",
    "    limite_inf = Q1 - 1.5 * IQR\n",
    "\n",
    "    return (limite_inf, limite_sup)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a48f163c-2898-4ea8-be53-52794ffab5ff",
   "metadata": {},
   "source": [
    "Un simple boolean indexing nous permet ensuite d’analyser séparément les données avec ou sans outlier :\n",
    "\n",
    "```python\n",
    "sup, inf = IQR_outliers(df.col) \n",
    "df_sans_outliers = df[((df.col < sup) & (df.col > inf))]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7476c00-6d97-451a-bd8a-e6305c66b1f7",
   "metadata": {},
   "source": [
    "#### Repérer les outliers à partir des écarts-types"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ebb7095-129d-4479-932f-5b4e7c221926",
   "metadata": {},
   "source": [
    "On peut fixer une limite en matière d’écart-type (par exemple fixer la limite à 3 écart-types). Cette méthode fonctionne bien pour les distributions normales  :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e71b76a0-b016-4beb-af20-e410dc1d481d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sd_outliers(df_col : pd.Series) -> tuple:\n",
    "    seuil = 3 # on peut aussi mettre le seuil en argument de la fonction\n",
    "    limite_sup = df_col.mean() + seuil * df_col.std()\n",
    "    limite_inf = df_col.mean() - seuil * df_col.std()\n",
    "\n",
    "    return (limite_inf, limite_sup)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2801614-e6e1-4161-abd4-24cb609f059d",
   "metadata": {},
   "source": [
    "#### Repérer les outliers à partir du z-score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04c0d56c-4528-4fae-b3d8-f5053afbf0d3",
   "metadata": {},
   "source": [
    "On peut aussi fixer une limite par rapport à la position dans la distribution, grâce au z-score :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb534835-1532-4f13-b604-1cdc01ce5616",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import zscore\n",
    "import numpy as np\n",
    "\n",
    "def z_outliers(df: pd.DataFrame, threshold: float) -> tuple:\n",
    "    z_scores = np.abs(zscore(df))\n",
    "    return df[(z_scores < threshold).all(axis=1)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d39f3365-325a-435f-934a-ccbdc2c1d6df",
   "metadata": {},
   "source": [
    "#### Autres méthodes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e1eb2ca-bc74-48c9-826f-b19b6a67d624",
   "metadata": {},
   "source": [
    "Dans certains cas, on peut aller plus loin dans l’élaboration des techniques pour identifier les outliers. Par exemple on peut choisir une distribution de référence (si l’on sait que nos données obéissent à cette loi), et estimer un seuil de probabilité pour cette distribution au delà de laquelle on considère que l’observation est trop improbable pour avoir eu lieu (c’est un peu ce que l’on fait ci-dessus avec des distribution normales). Mais on peut aussi utiliser des algorithmes de machine learning comme le k-mean clustering, et considérer que certaines observations qui sont trop éloignés des clusters qui auront été identifiés sont des outliers. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "031cd226-ee31-4dee-98c3-996b750ff5e6",
   "metadata": {},
   "source": [
    "### Gérer les outliers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1db8d9f0-afa3-4e9c-8ae2-e4b03ca983c2",
   "metadata": {},
   "source": [
    "1. Les supprimer simplement et proprement. La taille de l’échantillon est réduite, ce qui peu parfois poser problème.\n",
    "2. Les « winsoriser » (du nom du bio-statisticien [Charles Winsor](https://en.wikipedia.org/wiki/Charles_Winsor)). Il s’agit de remplacer (et non de supprimer) la valeur des outliers par des seuils (ou des plafonds). Par exemple si dans un ensemble de données qui s’étendent entre 12 (min) et 52 (max) on estime que les valeurs au dessus du 95e percentile sont des outliers, et que ce percentile commence à la valeur 43 par exemple, on va rammener toutes les valeurs au dessus de 43 à 43. Attention le processus doit être symétrique : il faut faire de même avec le 5e percentile, tout en étant vigilant car il faut modifier le même nombre de données de chaque côté de la distribution (ce qui peut n’être pas le cas si on ne considère que les percentiles, quand plusieurs données on la même valeur). La taille de l’échantillon reste intacte.\n",
    "3. Appliquer des transformations qui « normalisent » les distributions, comme des fonctions logarithmes, des fonctions racines, etc. Mais il faut avoir de bonnes raisons de le faire.\n",
    "4. Normaliser les données : parfois la normalisation (z-score) ramène l’essentiel des observations dans l’intervalle -3/+3 s.d., et les outliers se retrouvent à une « juste » place où leur influence est atténuée (en valeur absolue)\n",
    "5. Mettre en œuvre des modèles qui sont réputés plus [robustes](https://en.wikipedia.org/wiki/Robust_regression) à l’influence des outliers que les modèles classiques (les mégthodes d’estimation comme l’OLS y est très sensible).\n",
    "\n",
    "Dans tous les cas il faut être très prudent dans la gestion des outliers : on modifie des données qui rendent compte de la forme des distribution, notamment les queues, or elles peuvent jouer un rôle important dans les analyses statistiques que l’on veut mener (condition d’application, hypothèses…). C’est pour cela que notre décision dépend aussi du type d’outlier (cf. liste plus haut) qui vont conduire à des actions différentes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4366bac-531a-42b2-997b-2fc37cf421b2",
   "metadata": {},
   "source": [
    "## Récap : Valeurs manquantes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61c968d6-b450-43f5-ab13-4f5328b91654",
   "metadata": {},
   "source": [
    "Les valeurs manquantes peuvent avoir les même conséquences que les outliers sur nos analyses (mauvaise estimation de la distribution, mauvaise estimation des paramètres d’un modèle, effets sur la variance, etc.), mais aussi deux autres effets négatifs :\n",
    "\n",
    "- souvent les valeurs manquantes obéissent à un pattern (recueil de données défectueux, accident, difficultés en un lieu, une date, une cible, en particulier). Si ce pattern n’est pas explicité, il peut contaminer le modèle ou les analyses qui vont mal refléter la réalité (biais statistique)\n",
    "- s’il manque des données, c’est comme si notre échantillonage n’était pas complet, et donc entame notre capacité à faire ressortir ou identifier dans nos analyses des relations significatives entre les variables (perte de puissance statistique)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9bf8f2e-8aee-4366-a20a-ac84e89e0182",
   "metadata": {},
   "source": [
    "#### Type de valeurs manquantes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a9ef953-998c-4c21-867e-d7c04aebe8fd",
   "metadata": {},
   "source": [
    "1. des valeurs qui manquent de manière totalement aléatoire (incident dans un recueil ou la copie d’une donnée)\n",
    "2. des valeurs qui manquent de manière aléatoire (n’importe quelle valeur peu manquer), mais en fonction de la valeur d’une autre variable, qui elle est connue\n",
    "3. des valeurs qui manquent de non aléatoire (ce sont juste des valeurs données qui peuvent manquer, par exemple on ne peut pas enregistrer des valeurs supérieur à 10 et cette variable a eu des valeurs supérieures à 10)\n",
    "4. des valeurs manquante pour des questions (souvent dans les questionnaires), des mesures, ou encore des variables précises au sein des observations\n",
    "5. des enregistrement entiers qui manquent (pas seulement quelques variables, mais une ligne complète), par exemple un questionnaire rendu vide\n",
    "\n",
    "On constate qu’encore plus que pour les outliers, le contexte et le type de la valeur manquante vont être très importants pour choisir le traitement adapté. D’ailleurs pour plusieurs cas, à ce niveau du cours, nous n’avons pas les connaissances nécessaires pour mettre en œuvre certaines solutions. On les présente ici juste à titre documentaire. Y revenir quand vous serez mieux outillés en matière de statistiques et de modélisation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ab0b7f8-af30-4313-9886-f221e5db2d1f",
   "metadata": {},
   "source": [
    "#### Gérer les valeurs manquantes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1809d5d-6091-489f-af05-d5ed058b124f",
   "metadata": {},
   "source": [
    "Comme pour les outliers, il y a deux stratégies : la suppression ou le remplacement.\n",
    "\n",
    "1. supprimer tout enregistrement (ligne) où une valeur est manquante, avec la méthode ```df.dropna()```. Solution radicale, mais qui risque de diminuer drastiquement la taille de l’échantillon\n",
    "2. pour les valeurs manquantes totalemenet aléatoires (le cas (1) dans la liste ci-dessus), une bonne technique est de remplacer la valeur manquante par la moyenne ou le mode (la valeur la plus fréquente) de la variable concernée. On appelle ça une imputation. On peut le faire à la main (définir une fonction qui remplace les valeurs manquantes par, au choix, la moyenne ou le mode), mais la bibliothèque de machine learning [Scikit-Learn](https://scikit-learn.org/stable/index.html) dispose de tout un ensemble de méthodes pour cela – module ```sklearn.inpute```. Vous verrez cela dans le module machine learning, pour le moment on fera ces remplacements « à la main ». Cette technique est problématique par contre dans les situations où les valeurs manquantes sont liées à une autre variable (cas (2) et (3) ci-dessus).\n",
    "3. si on suspecte un pattern derrière les valeurs manquantes, on peut envisager de créer un modèle qui va permettre, à partir des valeurs que prennent les autres variables observées, de recréer les valeurs supposées de la variable qui fait défaut. On peut utiliser un modèle de régression, de corrélation, ou des modèles qui reconstruisent la distribution de la valeur manquante. Des modèles de clustering peuvent aussi intevenir (en repérant d’autres enregistrements qui « ressemblent » à celui où la valeur est absente, mais où elle est présente par contre et où on la recopiera). Il y a d’autres modèles plus complexes adaptés à cet objectif : *multivariate imputation through chained equations, pattern mixture model, predictive mean matching*…\n",
    "\n",
    "Pour ce cours d’introduction on se limitera aux deux premières solutions.\n",
    "Quelques conseils : d’abord rechercher si l’absence de valeur obéit à un pattern, la situation la plus favorable étant que ces absences soit totalement aléatoires. Ensuite se demander si la variable affectée est importante pour notre analyse ou pas (inutile de perdre du temps pour rien, avec une mesure de mauvaise qualité). C’est là que la connaissance métier, ou la connaissance du domaine est importante. De la même manière, ne pas négliger la recherche d’outlier, cette inspection des données doit être systématique et effectuée assez tôt dans l’exploration des données.\n",
    "\n",
    "**Gardez bien une trace de toutes les manipulations que vous effectuez**\n",
    "Vous aurez vraissemblablement à appliquer différentes méthodes : élimination dans certains, remplacement par des moyennes, idem pour les outliers, etc. Le nettoyage étant lui aussi un processus itératif, il est extrêmement important de rendre vos traitement reproductibles, et traçables (savoir ce que vous avez fait, dans quel ordre…). Donc conserver des logs !"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b2df428-bab4-45e8-bea0-f70438e8e215",
   "metadata": {},
   "source": [
    "## Méthodes pour l’EDA (dans l’ordre)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c42e402a-62c0-45d6-8090-d2db73aac2c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "tips_df = sns.load_dataset(\"tips\")\n",
    "tips_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ed9d960-1ce9-48e2-b4e2-60858aac79cf",
   "metadata": {},
   "source": [
    "Le but premier de l’EDA (*Exploratory Data Analysis*) est de se construire une bonne représentation et surtout compréhension des données en relevant des tendances et en détectant les anomalies. C’est la première étape indispensable de la création d’un modèle de *machine learning* ou de l’analyse de données. C’est à partir de là que vous formulerez les hypothèses (sur les relations entre variables) qui guideront votre travail et que vous allez tester.\n",
    "\n",
    "Opérationnellement, il s’agit donc d’une part d’établir des statistiques de bases qui pourront nous aider à décrire les données et les tendances que l’on observe, de générer des *dataviz* pertinentes, et d’autre part de préparer les données pour l’analyse à venir, notamment de les nettoyer (valeurs aberrantes, valeurs manquantes).\n",
    "\n",
    "Une poignée de méthodes sont à connaître absolument car elle nous permettont de réaliser la majeure partie de ce travail à l’aide de la bibliothèque `pandas`, et des bibliothèques de dataviz `seaborn` et `plotly`. Après avoir acquis ainsi une première compréhension des données à l’aide de ces méthodes, vous pourrez achever finement l’EDA en créant des opérations sur mesure pour vos données.\n",
    "\n",
    "* `df.info()` : la première méthode à appeler une fois que vous aurez chargé vos données sous la forme d’un DataFrame. Cette méthode vous permet d’un coup de disposer des informations suivantes :\n",
    "  \n",
    "    * la taille (*shape*) du tableau (lignes / colonnes)\n",
    "    * le nom de chaque colonne (nom de variables *a priori*)\n",
    "    * le nombre de valeurs non-nulles dans chaque colonne\n",
    "    * le datatype des valeurs dans chaque colonne\n",
    "    * la taille en mémoire du DataFrame "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06d232ef-6e3b-4aa7-bbf5-28e5345ce33c",
   "metadata": {},
   "outputs": [],
   "source": [
    "tips_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75f31ace-4144-4a65-aa36-7e0dcfdd3939",
   "metadata": {},
   "source": [
    "* pour les données numériques/quantitatives : `df.describe()`. Cette méhode va vous fournir les statistiques descriptives de base (nombre, moyenne, écart-type, médiane, min, max, quartiles…) permettant de se représenter la distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4500bf6-3f96-41f6-a8cc-98e61618df17",
   "metadata": {},
   "outputs": [],
   "source": [
    "tips_df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dee652cc-ad72-4a8e-b630-c56e12fa0d00",
   "metadata": {},
   "source": [
    "* Pour les variables catégorielles/qualitatives :  `df['colonne'].value_counts()`. Cette fonction permet de dénombrer rapidement le nombre d’élément dans chaque catégorie (qui sont alors énumérées par la même occasion). Cela remplace avantageusement le recours aux méthoddes `df.unique()̀` et `df.nunique()̀` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62153f90-1158-4b76-b976-070ad6300494",
   "metadata": {},
   "outputs": [],
   "source": [
    "tips_df['day'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c95572a0-1966-4503-8351-c04f65e2cea1",
   "metadata": {},
   "source": [
    "### Dataviz"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e09866a3-4596-4038-a47b-8ad6c4d14113",
   "metadata": {},
   "source": [
    "#### Basique\n",
    "\n",
    "* `df.plot()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d28b5e56-7326-4dd1-a41f-66ef896cf1de",
   "metadata": {},
   "outputs": [],
   "source": [
    "tips_df.plot();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63acc63c-7146-4ff6-a563-9d773e4f0852",
   "metadata": {},
   "source": [
    "* ̀`df['colonne'].hist(bins = nombre_de_barres)` permet d’avoir rapidement une idée de la distribution d’une variable quantitative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ede6e7d8-3df5-4baa-ac7c-77bd514695db",
   "metadata": {},
   "outputs": [],
   "source": [
    "tips_df['total_bill'].hist(bins=20);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71058671-dfbb-4bc1-8e47-256e316fc980",
   "metadata": {},
   "source": [
    "#### Avec `seaborn`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfe9ff19-8e82-4d5a-bf98-4c6afa537c34",
   "metadata": {},
   "source": [
    "* ̀`sns.histplot()` permet de visualiser une distribution avec en prime l’option `kde` qui permet d’avoir une estimation de la courbe de densité"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8155729-0dad-45b6-8cf7-4ac94cc2f39c",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot(tips_df['total_bill'], kde = True);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ed7352f-232e-4572-b170-9aba7b978874",
   "metadata": {},
   "source": [
    "#### Avec `plotly`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a2434df-a58f-42e8-b799-058ae031a98a",
   "metadata": {},
   "source": [
    "[plotly](https://plotly.com/python/) est une autre bibliothèque de dataviz qui met l’accent sur l’interactivité (elle forme avec Dash – basé sur Flask – un framework tout à fait adapté au dashboarding). Cette interactivité est intéressante pour l’exploration (possibilité de zoomer, etc.)\n",
    "Son module `plotly.express` permet de créer des figures complexes en une seule ligne de code, selon un format standardisé :\n",
    "`nom_de_la_fonction(dataframe,  x= ,  y= ,  title= ,  width= , height= )`, les dimensions étant données en pixel.\n",
    "\n",
    "[Cheat sheet Plotly](https://media.datacamp.com/legacy/image/upload/v1668605954/Marketing/Blog/Plotly_Cheat_Sheet.pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b12ee5a8-4125-4271-897d-620732a57c04",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install plotly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c1bc67c-3702-4fb2-949d-976d4dcfbcd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3cfee30-11b6-4ecc-af62-6c4ade7042c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "px.histogram(tips_df, x='total_bill', marginal='box', width=800, height=600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c3cbed7-efc8-42d7-b283-bc60208013fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from plotly import figure_factory as ff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1e9dbf0-bdd8-44ae-beae-9a562ab6ba50",
   "metadata": {},
   "outputs": [],
   "source": [
    "group_labels = ['total_bill']\n",
    "fig = ff.create_distplot([tips_df.total_bill], group_labels)\n",
    "fig.update_layout(width=800, height=600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e392fff4-e5ca-47f0-82e8-3328cecf3d22",
   "metadata": {},
   "outputs": [],
   "source": [
    "px.scatter(tips_df, \n",
    "           x='total_bill', \n",
    "           y='tip', \n",
    "           title='Tips x Total_bill', \n",
    "           width=600, \n",
    "           height=400)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7769e8ad-6d87-476b-898b-5428363b32ac",
   "metadata": {},
   "source": [
    "### Valeurs manquantes et aberrantes (outliers)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7180feee-fc98-4b58-8621-39c935827afc",
   "metadata": {},
   "source": [
    "* `df.count()` pour compter les valeurs non-manquantes (non NA) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ab27c3a-1e68-4cf3-90e0-0b40cc2af8bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "tips_df.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbda75c3-8f48-42f7-9a92-4f9534a90508",
   "metadata": {},
   "source": [
    "* `df.isnull()` ou `df.isna()` retournent un dataframe booléen où les valeurs `None` ou `np.NaN` sont à `True`. `df.notna()` fait l’opposé. Attention, les chaînes vides `''` ou des valeurs comme `np.inf` ne comptent pas comme des `NA` : il est important de bien inspecter les données pour voir comment identifier les valeurs manquantes (codes ? – chaînes ou valeurs numériques spécifiques, par exemple il n’est pas rare d’avoir des valeurs négatives là où elles ne devraient être que positives, chaînes vides ? etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "057dd0fa-78f9-44b7-a901-6ab63c061304",
   "metadata": {},
   "outputs": [],
   "source": [
    "tips_na_df = tips_df.copy()\n",
    "tips_na_df.iloc[tips_na_df.shape[0]-1,0] = np.nan # remplace par une valeur nulle\n",
    "tips_na_df.isnull()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c440e642-a87a-4062-a599-d430ac72e48c",
   "metadata": {},
   "source": [
    "* `df.dropna()` pour les éliminer. Sinon créer une fonction sur mesure pour remplacer par des valeurs (médiane, mode, etc.) en fonction de la situation  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6563c957-f9cf-4522-90e1-6e77353ad5c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "tips_na_df.dropna(axis = 0) # attention à l’axe ! axis = 0 par défaut. Attention au inplace = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83baa8e9-630c-4c53-a3d1-8fe00ed94fdd",
   "metadata": {},
   "source": [
    "* des fonctions pour récupérer les index des valeurs nulles (à modifier pour repérer des valeurs nulles autres que `None` ou `np.nan`) :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87cb0bb7-06e2-42cf-b887-dfc4540718b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_na_index(df):\n",
    "    return np.where(df.isna())\n",
    "\n",
    "get_na_index(tips_na_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7978c5d2-17f8-477d-a563-e2d5e78cf340",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_na_values(df, idx):\n",
    "    return [df.iloc[i,j] for i,j in zip(*idx)] # on utilise l’opérateur de déballage * pour « dézipper »\n",
    "\n",
    "get_na_values(tips_na_df, get_na_index(tips_na_df))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f30bbdef-988d-42ff-8545-700cc13369a3",
   "metadata": {},
   "source": [
    "* note : voir [cette page](https://www.geeksforgeeks.org/zip-in-python/) si vous n’êtes pas à l’aise avec la fonction `zip()`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b25581a6-8e4a-4e53-8f97-29d87efe8601",
   "metadata": {},
   "source": [
    "* `px.box()` avec `plotly` pour créer des boîtes à moustache où on peut lire facilement les valeurs :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7f5c103-8dad-4936-98cf-ad16a8317f59",
   "metadata": {},
   "outputs": [],
   "source": [
    "px.box(tips_df, y=\"total_bill\", width=600, height=400)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99ea84a5-c7ad-4865-9a10-6b8d648f0509",
   "metadata": {},
   "source": [
    "* repérer les outliers à l’aide d’une fonction sur mesure (méthode des IQR, voir le récap de la séance 3 pour d’autres méthodes : écart-type, standardisation…)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "825fb588-20a1-4513-a72e-2e8a5327bcfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Répérer les outliers à partir des IQR\n",
    "def IQR_outliers(df_col: pd.Series) -> tuple:\n",
    "    Q1 = df_col.quantile(0.25)\n",
    "    Q3 = df_col.quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    limite_sup = Q3 + 1.5 * IQR\n",
    "    limite_inf = Q1 - 1.5 * IQR\n",
    "\n",
    "    return (limite_inf, limite_sup)\n",
    "\n",
    "IQR_outliers(tips_df.total_bill)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abebc07b-9b4b-4d43-814e-4ab2694576b9",
   "metadata": {},
   "source": [
    "### Tendances"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18069710-159e-462d-b84f-97c6f2cb0dbb",
   "metadata": {},
   "source": [
    "* `df.corr()` une table de corrélation nous permet déjà d’émettre des hypothèses sur des relations entre variables quantitatives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ddda658-cdd7-47db-a880-659279b1268d",
   "metadata": {},
   "outputs": [],
   "source": [
    "tips_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cbba3fa-2a1d-4ffb-99c0-fb116d3e3009",
   "metadata": {},
   "outputs": [],
   "source": [
    "tips_df[['total_bill', 'tip', 'size']].corr()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cecadbf4-d48d-4bfa-9587-05d2f5b8af0c",
   "metadata": {},
   "source": [
    "* `sns.heatmap()` nous permet de visualiser plus facilement les corrélations plt.figure(figsize=(11,7))\n",
    "sns.heatmap(orders.select_dtypes(exclude = [\"object\"]).corr(), \n",
    "            cmap='coolwarm', \n",
    "            annot = True, \n",
    "            annot_kws={\"size\": 12})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92bf56f4-2c6d-4ab9-8a2e-f0dfa1cc30c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(6,5))\n",
    "sns.heatmap(tips_df[['total_bill', 'tip', 'size']].corr(), \n",
    "            cmap='coolwarm', \n",
    "            annot = True, # affiche les valeurs dans les cellules\n",
    "            annot_kws={\"size\": 12}); # taille des chiffres dans les cellulles"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d8d4685-3f65-4f2f-ab66-0876f5621ee4",
   "metadata": {},
   "source": [
    "* `sns.pairplot()` vous permet de tracer des nuages de points en considérant des variables deux à deux (un peu comme un tableau de corrélation de la dataviz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80b55e74-24c9-4353-bec7-ce18d7cd993b",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(tips_df, height=2);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0001cc22-8461-4927-be88-6338cc49f5d2",
   "metadata": {},
   "source": [
    "* lorsque l’on a repéré des relations intéressantes, on peut les observer plus en détail avec :\n",
    "    * `sns.jointplot()` pour avoir une idée des distribution dans un nuage de point\n",
    "    * `sns.regplot()` pour la relation entre variable quantitative et une estimation de la tendance (droite de regression)\n",
    "    * `sns.countplot()` avec l’argument `hue=` pour mettre en relation des variables catégorielles\n",
    "    * `sns.stripplot()` ou `sns.swarmplot()` pour mettre en relation des variables quantitatives et catégorielles, en améliorant la visibilité de la dispersion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba1a3c0d-87f1-4bfe-8669-07172d00ff79",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.jointplot(x='total_bill', y='tip', data=tips_df);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68d4910b-a855-4b54-94c4-e71fe36c4e2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.regplot(x='total_bill', y='tip', data=tips_df);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8004c407-6b53-41dc-9e07-110bed29e3f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.countplot(x='day', data=tips_df, hue='sex');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fbcc7a2-55a5-41e4-9753-2a9bfbada3e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.stripplot(x='time', y='total_bill', data=tips_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54ec33ba-6ad0-4807-98bf-37df8575720b",
   "metadata": {},
   "source": [
    "* l’argument `trendline` dans un graphe de type `px.scatter()`, permet également d’afficher une tendance avec `plotly`, il faut alors indiquer le type d’estimation voulu. Par exemple, pour une régression un estimateur courant est `ols` (on verra cela en détail dans le cours sur la régression). Cette fonction nécessite la bibliothèque `statsmodels`. `plotly` permet alors d’afficher la valeurs des paramètres estimés et d’autres indicateurs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "953dff6f-b76c-401f-b473-67be58af3af1",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install statsmodels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6c75779-e740-4d30-a3b0-b1780e6a151d",
   "metadata": {},
   "outputs": [],
   "source": [
    "px.scatter(tips_df, x='total_bill',y='tip',trendline='ols', width=600, height=400)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68c54ddf-af1c-4143-81f9-810a534c2122",
   "metadata": {},
   "source": [
    "### Automatiser l’exploration : `ydata-profiling`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f7828b1-a3ab-4abf-adca-256eba4a10eb",
   "metadata": {},
   "source": [
    "https://pypi.org/project/ydata-profiling/\n",
    "\n",
    "* créer un rapport d’analyse exploratoire de dataframes, généré automatiquement\n",
    "* exporter le rapport dans différents formats (HTML, JSON…)\n",
    "* **Note : ydata est très chatouilleux sur les dépendances, je conseille fortement de mettre en place un environnemnet virtuel dédié, ou d’être très attentifs sur les versions des dépendances (déjà) installées**\n",
    "\n",
    "Exemple : [EDA du fameux dataset « Titanic »](https://docs.profiling.ydata.ai/latest/examples/titanic/titanic_report.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6022615e-a2e8-499f-9263-60e4aaca1a42",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!deactivate\n",
    "#!python3 -m venv ydata\n",
    "#!source ydata/bin/activate\n",
    "#!pip install ydata-profiling\n",
    "from ydata_profiling import ProfileReport\n",
    "!mkdir reports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "172c380e-da2e-4e38-a7b7-b942915bcb4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "profile = ProfileReport(tips_df, title = 'tips')\n",
    "profile.to_file('reports/tips_report.html')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "495f44a2-7a4e-4db9-bc3e-ee2e64d541c4",
   "metadata": {},
   "source": [
    "## Exercices / Exemples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28280b1d-7a2c-4d57-9dfd-7b87bd0a3ca6",
   "metadata": {},
   "source": [
    "*Vous n’êtes pas obligés de faire les explorations dans l’ordre : suivez votre inspiration. Mais essayez de tout faire : plus vous explorerez de situations différentes, plus vous serez familier avec ces manipulations.*\n",
    "\n",
    "Voici quelques jeux de données mis à disposition librement, et qui peuvent servir de support pour mettre en œuvre les techniques d’EDA :\n",
    "\n",
    "1. Synthétisez les données avec des statistiques descriptives\n",
    "2. Repérez si ces données doivent être nettoyées (valeurs manquantes ? outliers ?)\n",
    "3. Repérez des corrélations, regardez si vous pouvez faire émerger des patterns par de simples visualisations\n",
    "4. N’hésitez pas à simplifier le problème en faisant des hypothèses sur les variables qui vous semblent « à retenir » (lisez bien les métadonnnées si elles sont disponibles)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb9583c1-18d8-4cb1-b80a-e230d3cfcc8d",
   "metadata": {},
   "source": [
    "### Jeu de données 1 : house prices"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f5035b7-910e-4dcb-9aef-76ff466560d4",
   "metadata": {},
   "source": [
    "Le prix de vente de biens immobiliers et de nombreuses variables :\n",
    "[house prices](https://www.kaggle.com/c/house-prices-advanced-regression-techniques/data)\n",
    "\n",
    "Ce jeux de données est proposé dans le cadre d’un sujet de machine learning, nous n’irons pas aussi loin. Nous nous contenterons d’explorer ces données. Aussi seuls trois fichiers nous intéressent : train.csv et test.csv qu’il faudra assembler en un seul dataframe de données, et le fichier texte qui l’accompagne pour bien comprendre les données, ce qui vous permettra de formuler des hypothèses pour orienter votre exploration.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05b5e0eb-445f-420d-a009-5af4a801f885",
   "metadata": {},
   "source": [
    "### Jeu de données 2 : loan predication"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4db72c52-2145-472c-93fa-3706c6a9ca5e",
   "metadata": {},
   "source": [
    "Voilà un jeu de données intéressant pour étudier la questions des outliers, des transformations de distributions… [loan predication](https://www.kaggle.com/datasets/ninzaami/loan-predication/data)\n",
    "\n",
    "Le problème est de mettre en relation des caratéristiques d’individus, et la susceptibilité de leur attribuer un prêt.\n",
    "\n",
    "Réalisez toutes les explorations demandées, mais notez qu’à un moment, selon ce que vous observerez graphiquement, ce jeu de données se prête bien à une transformation par une fonction logarithme ;)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf01fe9a-4164-4b74-b6a5-ede53d7c82a7",
   "metadata": {},
   "source": [
    "### Jeu de données 3 : pétrole brut et gaz naturel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da389711-a29b-4925-823c-ca2f4bbf2dff",
   "metadata": {},
   "source": [
    "Ce jeu de données indique la production US de gaz et de pétrole de juin 2008 à juin 2018 : [us-oil-and-gas-production](https://www.kaggle.com/datasets/djzurawski/us-oil-and-gas-production-june-2008-to-june-2018)\n",
    "\n",
    "Ces données font intervenir des informations temporelles.\n",
    "\n",
    "En plus des explorations classiques ce jeu de données à pour intérêt :\n",
    "\n",
    "* il vous faudra convertir des objects (string) en timestamp pour pouvoir exploiter ces données\n",
    "* avec des ```.groupby()``` essayez de calculer et représenter les productions annuelles de gaz et de pétrole\n",
    "* de même présentez graphiquement la production par États\n",
    "* assemblez en un seul dataframe synthétique, par année, la production de gaz **et** de pétrole, pour l’ensemble des US seulement, et un autre, État par État."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14158ab9-ee0f-4ec3-9824-fc6285a0e8ff",
   "metadata": {},
   "source": [
    "### Jeu de données 4 : émissions de CO2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd87d847-d025-4d82-8644-332b19c8dbea",
   "metadata": {},
   "source": [
    "Ce jeu de données répertorie pour des véhicules de différents modèles la quantité de CO2 émise : [vehicle-co2-emissions-dataset](https://www.kaggle.com/datasets/brsahan/vehicle-co2-emissions-dataset/data)\n",
    "\n",
    "L’occasion de faire quelques visualisation très informatives !"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9547d611-2ec8-47ed-af81-5f65de4c48af",
   "metadata": {},
   "source": [
    "### Jeu de données 5 : des vraies données bien sales"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f0f829a-cd20-4c3d-9f7e-b49961ef8d54",
   "metadata": {},
   "source": [
    "Si vous trouvez les jeux de données précédents trop simples et qu’ils ne demandent pas assez de travail de nettoyage, etc., voici un jeu de données (réelles) qui vous donnera plus de fil à retordre : [Olist dataset](https://www.kaggle.com/datasets/olistbr/brazilian-ecommerce)\n",
    "\n",
    "Un bon exercice pour apprendre à merger des données, et à créer des métriques.\n",
    "\n",
    "1. Observez bien le schéma de la base de données\n",
    "2. Essayez à partir de là de créer un dataframe unique (il va falloir faire des ```.merge(), .groupby()```, etc.) qui contient toutes les données intéressantes, selon des hypothèses que vous pourrez faire : par exemple, y a-t-il une corrélation entre la vitesse à laquelle un consommateur reçoit un produit et l’appréciation qu’il laisse à un vendeur ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bece24d5-fa4f-4101-b075-98ce22a40d9b",
   "metadata": {},
   "source": [
    "### Présentez un jeu de données qui a attiré votre intérêt pour le projet de ce module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d33f86b-960f-4b27-82b1-7469c3dedc6d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "333d77c2-e7d1-4007-8975-82ff910678f5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
